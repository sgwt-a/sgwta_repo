# oreilly ゼロから作る Deep Learning

## サンプルコード

[github](https://github.com/oreilly-japan/deep-learning-from-scratch)

## 要約

### 2章 パーセプトロン

### パーセプトロンの振る舞い
- y = 0 ( b + w1*x1 + w2*x2 <= 0 )
- y = 1 ( b + w1*x1 + w2*x2 > 0 )

パーセプトロンによって AND, NAND, OR の振る舞いを表現することができる。
しかし、XORは（単層では）表現できない。
多層にすることで（AND, NAND, ORを組み合わせることで）XORを表現できる。


### 3章 ニューラルネットワーク

#### 活性化関数
- ニューラルネットワーク = ( パーセプトロン -> 活性化関数 ) の繰り返し
- 活性化関数は非線形関数である必要がある。
   - 線形関数は、どんなに層を深くしても、それと同じことを行う隠れ層のないネットワークが必ず存在する。
   - 線形関数 h(x)=cx を活性化関数とした3層ネットワークの場合を考えると、出力は y(x) = h(h(h(x)))となるが、これは y(x)=ax (a=c^3) で表現できてしまうため多層のメリットがない。

- ステップ関数（x<0でy=0, x>=0でy=1）
   - 微分がほぼ常に(x=0以外で)0 -> 学習に向かない
- シグモイド関数（x=-infでy=0, x=0でy=0.5, x=infでy=1）
- ReLU関数（x<0でy=0, x>=0でy=x）

#### 出力層の活性化関数
- 恒等関数・・・回帰問題
- ソフトマックス関数・・・分類問題
※学習として微分したときに (t - x) という綺麗な形になるように設計されている。


### 4章 ニューラルネットワークの学習

#### 損失関数
- 2乗和誤差：出力層の活性化関数が恒等関数のときに使用する（微分が (t-x) の綺麗な形になる）
- 交差エントロピー誤差：出力層の活性化関数がソフトマックス関数のときに使用する（微分が (t-x) の綺麗な形になる）

### 5章 誤差逆伝播法

勾配（微分）を求める方法
- 数値微分：{f(x+h) - f(x-h)} / 2h
- 解析的に微分：f(x) = x^2 -> df/dx = 2x

解析的な方が都度微分計算が不要なので計算コストが低い。
計算結果はほぼ一致している。（無視できる誤差程度しか差がない）

### 6章 学習のテクニック

#### 重みパラメータの更新
- SDG(確率的勾配降下法)
   - W = W - η*(dL/dW)
   - 欠点：関数の形状が等方的でない場合、勾配方向が誤差の最小値と異なるため、探索経路の効率が悪い。
- Momentum
   - v = av - η*(dL/dW); W = W + v
   - vは物理でいう「速度」に対応する。傾きが小さい方向に対しても常に勾配があるため速度が増し、収束が早くなる。
- AdaGrad：
   - h = h + ( dL/dW ・ dL/dW ); W = W - η*(1/sqrt(h))*(dL/dW)
   - 学習率を減衰させる。（最初は大きく学習し、次第に小さく学習する）
   - 大きく更新された要素は学習率が小さくなる（減衰する）。
- Adam
   - AdaGrad + Momentum
   - 両方のメリットを含む


どれがよいかは、ニューラルネットワークの構造（層の深さなど）、ハイパーパラメータなどによって変化する。
ただ、一般的にはSDGよりは他3つの方が学習が速く、最終的な認識精度も高い。

#### 重みパラメータの初期値
- 重みが均一（極端な例だと、全て0の場合）だと、全ての重みが同じように更新されるため、ノードが複数ある意味がなくなってしまう。
- 活性化関数の出力値がばらつきを持つような重みが望ましい。

初期値の種類
- Xavierの初期値
   - 前層のノードの個数をnとしたとき、(1/sqrt(n))の標準偏差を持つ分布
   - ノードの個数が多いほど、重みの値は小さくなる。
   - 活性化関数が線形であることを前提に導いた結果。
      - シグモイド関数やtanh関数は左右対称で中央付近が線形関数とみなせるため、Xavierの初期値が適している。
- Heの初期値
   - 前層のノードの個数をnとしたとき、(sqrt(2/n))の標準偏差を持つ分布
   - 活性化関数にReLU関数を用いたとき。
   - 直感的な解釈としては、ReLUは負の領域で0となるため、より広がりを持たせるために倍の係数が必要になる。
