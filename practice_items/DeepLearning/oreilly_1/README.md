# oreilly ゼロから作る Deep Learning

## サンプルコード

[github](https://github.com/oreilly-japan/deep-learning-from-scratch)

## 要約

### 2章 パーセプトロン

### パーセプトロンの振る舞い
- y = 0 ( b + w1 × x1 + w2 × x2 <= 0 )
- y = 1 ( b + w1 × x1 + w2 × x2 > 0 )

パーセプトロンによって AND, NAND, OR の振る舞いを表現することができる。
しかし、XORは（単層では）表現できない。
多層にすることで（AND, NAND, ORを組み合わせることで）XORを表現できる。


### 3章 ニューラルネットワーク

#### 活性化関数
- ニューラルネットワーク = ( パーセプトロン -> 活性化関数 ) の繰り返し
- 活性化関数は非線形関数である必要がある。
   - 線形関数は、どんなに層を深くしても、それと同じことを行う隠れ層のないネットワークが必ず存在する。
   - 線形関数 h(x)=cx を活性化関数とした3層ネットワークの場合を考えると、出力は y(x) = h(h(h(x)))となるが、これは y(x)=ax (a=c^3) で表現できてしまうため多層のメリットがない。

- ステップ関数（x<0でy=0, x>=0でy=1）
   - 微分がほぼ常に(x=0以外で)0 -> 学習に向かない
- シグモイド関数（x=-infでy=0, x=0でy=0.5, x=infでy=1）
- ReLU関数（x<0でy=0, x>=0でy=x）

#### 出力層の活性化関数
- 恒等関数・・・回帰問題
- ソフトマックス関数・・・分類問題
※学習として微分したときに (t - x) という綺麗な形になるように設計されている。


### 4章 ニューラルネットワークの学習

#### 損失関数
- 2乗和誤差：出力層の活性化関数が恒等関数のときに使用する（微分が (t-x) の綺麗な形になる）
- 交差エントロピー誤差：出力層の活性化関数がソフトマックス関数のときに使用する（微分が (t-x) の綺麗な形になる）

### 5章 誤差逆伝播法

勾配（微分）を求める方法
- 数値微分：{f(x+h) - f(x-h)} / 2h
- 解析的に微分：f(x) = x^2 -> df/dx = 2x

解析的な方が都度微分計算が不要なので計算コストが低い。
計算結果はほぼ一致している。（無視できる誤差程度しか差がない）

### 6章 学習のテクニック

#### 重みパラメータの更新
- SDG(確率的勾配降下法)
   - W = W - η×(dL/dW)
   - 欠点：関数の形状が等方的でない場合、勾配方向が誤差の最小値と異なるため、探索経路の効率が悪い。
- Momentum
   - v = av - η×(dL/dW); W = W + v
   - vは物理でいう「速度」に対応する。傾きが小さい方向に対しても常に勾配があるため速度が増し、収束が早くなる。
- AdaGrad：
   - h = h + ( dL/dW ・ dL/dW ); W = W - η*(1/sqrt(h))*(dL/dW)
   - 学習率を減衰させる。（最初は大きく学習し、次第に小さく学習する）
   - 大きく更新された要素は学習率が小さくなる（減衰する）。
- Adam
   - AdaGrad + Momentum
   - 両方のメリットを含む


どれがよいかは、ニューラルネットワークの構造（層の深さなど）、ハイパーパラメータなどによって変化する。
ただ、一般的にはSDGよりは他3つの方が学習が速く、最終的な認識精度も高い。

#### 重みパラメータの初期値
- 重みの初期値の標準偏差が小さい場合（重みが均一）、活性化関数の出力値がどのノードもほぼ同じになり、全ての重みが同じように更新されるため、ノードが複数ある意味がなくなってしまう。
- 重みの初期値の標準偏差が大きい場合、
   - シグモイド関数：活性化関数の入力値の絶対値が大きいため、勾配が小さく、勾配消失になる。
   - ReLU関数：逆伝播を考える際に各層の勾配を掛け合わせたときに重みのかけ合わせが非常に大きくなる場合がある。勾配爆発。
- 活性化関数の出力値がばらつきを持つような重みが望ましい。

初期値の種類
- Xavierの初期値
   - 前層のノードの個数をnとしたとき、(1/sqrt(n))の標準偏差を持つ分布
   - ノードの個数が多いほど、重みの値は小さくなる。
   - 活性化関数が線形であることを前提に導いた結果。
      - シグモイド関数やtanh関数は左右対称で中央付近が線形関数とみなせるため、Xavierの初期値が適している。
- Heの初期値
   - 前層のノードの個数をnとしたとき、(sqrt(2/n))の標準偏差を持つ分布
   - 活性化関数にReLU関数を用いたとき。
   - 直感的な解釈としては、ReLUは負の領域で0となるため、より広がりを持たせるために倍の係数が必要になる。

#### Batch Normalization
重みの初期値は、各層の出力値が適度な広がりを持つように設定することが大事だった。  
そこで、各層で「強制的に」分布を調整できないか？  
→　Batch Normalization  

メリット
- 学習を速く進行できる（学習係数を大きくすることができる）
- 初期値にそこまで依存しない（初期値に対してそこまで神経質にならなくてよい）
- 過学習を抑制できる（DropOutなどの必要性を減らす）

使い方  
活性化関数の前に実施する。  
Affine -> Batch Normalization -> ReLU  

処理  
1. データの分布が平均"0"、分散"1"になるように正規化を行う。  
1. 正規化されたデータに対して、固有のスケールとシフトで変換を行う。（Lsbを掛けてOffsetを足す。）
   - ネットワークが元の分布に戻れる柔軟性を持つため。（Lsb=標準偏差、Offset=平均のときは元の分布に戻る）
   - 正規化だけだとReLUの特徴が失われる。（平均が"0"だと半数のReLUの出力が"0"になる）

#### 過学習
原因
- パラメータを大量に持ち、表現力が高いモデルであること（訓練データに最適化してしまう）
- 訓練データが少ないこと

対策
- Weight decay
   - 大きな重みを持つことに対してペナルティを課す仕組み
   ※重みパラメータが大きな値を取ることによって過学習が発生することが多くあるため
   - 重みの2乗ノルム（1/2*(係数)*W^2）を損失関数に加算する
- Dropout
   - ニューロン（ノード）をランダムに消去しながら学習する手法
   - 訓練時：データが流れるたびに消去するニューロンをランダムに選択する
   - テスト時：ニューロンを消去せずにすべてのニューロンを伝達するが、各ニューロンの出力に対して、訓練時に消去した割合を乗算して出力する。

### 7章 畳み込み
- パディング
   - 出力サイズを調整するため
- プーリング
   - 種類
      - Maxプーリング（画像認識の主流）
      - Averageプーリング
   - 入力データの微小なズレに対してロバストになる

畳み込みの演算に計算コストがかかる（全体の約90%以上）ため、im2colなどで計算の効率化を工夫する必要がある。　　
層が深くなるにつれてニューロンは単純な形状から高度な情報に変化する。つまり、モノの意味を理解するようになる。  

### 8章 ディープラーニングの学習
認識精度を向上させる工夫
- Data Augmentation（データ拡張）
    - 入力データに対して回転や縦横方向の移動などの微小な変化を与え、画像枚数を増やすこと


ネットワークの層を深くするメリット
- より少ないパラメータで同じレベルの表現力を達成できる。  
例）  
5×5のフィルターで畳み込み演算をすることと、3×3のフィルターで2回畳み込み演算することは、入力データの同じ領域に対して演算しているため同等である。  
パラメータ数は5×5フィルター=25個、3×3フィルター×2=18個となり、後者の方が少ない。
- 活性化関数の回数も増え、非線形の力が加わるため、表現力が増す。
